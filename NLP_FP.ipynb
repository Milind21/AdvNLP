{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNH1nt-RbpZ0",
        "outputId": "8172a5ce-5de5-4dd1-9614-78adb5a2fef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bSU-o3s9WT-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "\n",
        "#read the datasets\n",
        "# train_df = pd.read_csv(\"./train.csv\")\n",
        "# val_df = pd.read_csv(\"./val.csv\")\n",
        "# test_df = pd.read_csv(\"./test.csv\")\n",
        "df = pd.read_csv(\"./df2.csv\")\n",
        "df = df.rename(columns={\"Unnamed: 0\": \"label_id\"})\n",
        "text = df.text.values\n",
        "labels = df.label_id.values\n",
        "# train_df=train_df.rename(columns={\"Unnamed: 0\": \"label_id\"})\n",
        "# val_df=val_df.rename(columns={\"Unnamed: 0\": \"label_id\"})\n",
        "# test_df=test_df.rename(columns={\"Unnamed: 0\": \"label_id\"})\n",
        "# train_text = train_df.text.values\n",
        "# train_labels = train_df.label_id.values\n",
        "# val_text = val_df.text.values\n",
        "# val_labels = val_df.label_id.values\n",
        "# test_text = test_df.text.values\n",
        "# test_labels = test_df.label_id.values\n",
        "# train_labels = train_labels.astype(float)\n",
        "'''for idx in range(len(train_labels)):\n",
        "    train_labels[idx] = train_labels[idx].astype('float32')\n",
        "print(type(train_labels))\n",
        "print(type(train_labels[0]))'''\n",
        "\n",
        "#tokenize\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)\n",
        "\n",
        "#helper function to check tokenization of sentences\n",
        "def print_rand_sentence():\n",
        "  idx = random.randint(0, len(text)-1)\n",
        "  table = np.array([tokenizer.tokenize(text[idx]), tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[idx]))]).T\n",
        "  print(tabulate(table, headers = ['Tokens', 'Token IDs'], tablefmt = 'fancy_grid'))\n",
        "\n",
        "# print_rand_sentence()\n",
        "\n",
        "#tokenize the corpus\n",
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(input_text,add_special_tokens = True,  max_length = 64,padding = 'max_length',\n",
        "                        return_attention_mask = True,return_tensors = 'pt',truncation=True)\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)\n",
        "# print(token_id[196])\n",
        "\n",
        "# helper function to check encoding of sentences\n",
        "def print_rand_sentence_encoding():\n",
        "  index = random.randint(0, len(text) - 1)\n",
        "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "  token_ids = [i.numpy() for i in token_id[index]]\n",
        "  attention = [i.numpy() for i in attention_masks[index]]\n",
        "  table = np.array([tokens, token_ids, attention]).T\n",
        "  print(tabulate(table, headers = ['Tokens', 'Token IDs', 'Attention Mask'],tablefmt = 'fancy_grid'))\n",
        "\n",
        "# print_rand_sentence_encoding()\n",
        "\n",
        "#create dataset\n",
        "ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "train_i, val_i = train_test_split(np.arange(len(labels)),test_size = ratio,shuffle = True)\n",
        "# Train and validation sets\n",
        "# train = TensorDataset(token_id[train_i], attention_masks[train_i], labels[train_i])\n",
        "train = TensorDataset(token_id[:], attention_masks[:], labels[:])\n",
        "val = TensorDataset(token_id[val_i], attention_masks[val_i], labels[val_i])\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(train,sampler = RandomSampler(train),batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(val,sampler = SequentialSampler(val),batch_size = batch_size)\n",
        "\n",
        "#Main Training\n",
        "#Define eval metrics\n",
        "def b_tp(preds, labels):\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "def b_fp(preds, labels):\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "def b_tn(preds, labels):\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "def b_fn(preds, labels):\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity\n",
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased',num_labels = len(labels),output_attentions = False,output_hidden_states = False,)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5,eps = 1e-08)\n",
        "\n",
        "\n",
        "#Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# print(device)\n",
        "epochs = 16\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "    model.eval() \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0DBnA-Bf9jPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00daa089-9232-4cfc-85c3-b7acdf4304f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "P-cgH0e8JNqn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6WjuR_c_xCI",
        "outputId": "5f6c0c26-b132-42de-82e1-c59ea9190898"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./df2.csv\")\n",
        "df = df.rename(columns={\"Unnamed: 0\": \"label_id\"})"
      ],
      "metadata": {
        "id": "mpgSd2C3BKNR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "F11CsViMCYJn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df.text.values\n",
        "labels = df.category.values"
      ],
      "metadata": {
        "id": "uQ5FJqFiCcmH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id = {}\n",
        "id_to_label = {}\n",
        "j = 0\n",
        "for i in np.unique(labels):\n",
        "  label_to_id[i] = j\n",
        "  id_to_label[j] = i\n",
        "  j += 1"
      ],
      "metadata": {
        "id": "B4Tz4jo8KhPE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBgcOy5OL20t",
        "outputId": "7831f073-0a77-4b1e-aeef-8744ae2afa49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'African daisy': 0,\n",
              " 'African violet': 1,\n",
              " 'Dahlia': 2,\n",
              " 'Douglas-fir': 3,\n",
              " 'English walnut': 4,\n",
              " 'Jerusalem cherry': 5,\n",
              " 'Persian violet': 6,\n",
              " 'alfalfa': 7,\n",
              " 'almond': 8,\n",
              " 'anemone': 9,\n",
              " 'animal': 10,\n",
              " 'apple': 11,\n",
              " 'apricot': 12,\n",
              " 'asparagus': 13,\n",
              " 'avocado': 14,\n",
              " 'azalea': 15,\n",
              " 'banana and plantain': 16,\n",
              " 'barley': 17,\n",
              " 'beet': 18,\n",
              " 'bellflower': 19,\n",
              " 'black walnut': 20,\n",
              " 'bleeding heart': 21,\n",
              " 'butterfly flower': 22,\n",
              " 'cacao': 23,\n",
              " 'caneberries': 24,\n",
              " 'canola': 25,\n",
              " 'carnation': 26,\n",
              " 'carrot': 27,\n",
              " 'cassava': 28,\n",
              " 'cattleya': 29,\n",
              " 'chickpea': 30,\n",
              " 'cineraria': 31,\n",
              " 'citrus': 32,\n",
              " 'coconut palm': 33,\n",
              " 'coffee': 34,\n",
              " 'common bean': 35,\n",
              " 'cotton': 36,\n",
              " 'crucifer': 37,\n",
              " 'cucurbit': 38,\n",
              " 'cyclamen': 39,\n",
              " 'date palm': 40,\n",
              " 'durian': 41,\n",
              " 'elm': 42,\n",
              " 'flax': 43,\n",
              " 'foliage plant': 44,\n",
              " 'fuchsia': 45,\n",
              " 'geranium': 46,\n",
              " 'grape': 47,\n",
              " 'hazelnut': 48,\n",
              " 'hemp': 49,\n",
              " 'holiday cacti': 50,\n",
              " 'hop': 51,\n",
              " 'hydrangea': 52,\n",
              " 'impatiens': 53,\n",
              " 'kalanchoe': 54,\n",
              " 'lentil': 55,\n",
              " 'lettuce': 56,\n",
              " 'lisianthus': 57,\n",
              " 'maize': 58,\n",
              " 'mango': 59,\n",
              " 'mimulus, monkey-flower': 60,\n",
              " 'mint': 61,\n",
              " 'mustard': 62,\n",
              " 'oats': 63,\n",
              " 'papaya': 64,\n",
              " 'pea': 65,\n",
              " 'peach and nectarine': 66,\n",
              " 'peanut': 67,\n",
              " 'pear': 68,\n",
              " 'pearl millet': 69,\n",
              " 'pecan': 70,\n",
              " 'pepper': 71,\n",
              " 'pigeonpea': 72,\n",
              " 'pineapple': 73,\n",
              " 'pistachio': 74,\n",
              " 'pocketbook plant': 75,\n",
              " 'poinsettia': 76,\n",
              " 'potato': 77,\n",
              " 'primula': 78,\n",
              " 'red clover': 79,\n",
              " 'rhododendron': 80,\n",
              " 'rice': 81,\n",
              " 'rose': 82,\n",
              " 'rye': 83,\n",
              " 'safflower': 84,\n",
              " 'sapphire flower': 85,\n",
              " 'sorghum': 86,\n",
              " 'soybean': 87,\n",
              " 'spinach': 88,\n",
              " 'strawberry': 89,\n",
              " 'sugarcane': 90,\n",
              " 'sunflower': 91,\n",
              " 'sweetgum': 92,\n",
              " 'sweetpotato': 93,\n",
              " 'sycamore': 94,\n",
              " 'tea': 95,\n",
              " 'tobacco': 96,\n",
              " 'tomato': 97,\n",
              " 'verbena': 98,\n",
              " 'wheat': 99,\n",
              " 'wild rice': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT0Ovoz5L6zF",
        "outputId": "b7136b02-d175-4e40-956d-c758e7904307"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'African daisy',\n",
              " 1: 'African violet',\n",
              " 2: 'Dahlia',\n",
              " 3: 'Douglas-fir',\n",
              " 4: 'English walnut',\n",
              " 5: 'Jerusalem cherry',\n",
              " 6: 'Persian violet',\n",
              " 7: 'alfalfa',\n",
              " 8: 'almond',\n",
              " 9: 'anemone',\n",
              " 10: 'animal',\n",
              " 11: 'apple',\n",
              " 12: 'apricot',\n",
              " 13: 'asparagus',\n",
              " 14: 'avocado',\n",
              " 15: 'azalea',\n",
              " 16: 'banana and plantain',\n",
              " 17: 'barley',\n",
              " 18: 'beet',\n",
              " 19: 'bellflower',\n",
              " 20: 'black walnut',\n",
              " 21: 'bleeding heart',\n",
              " 22: 'butterfly flower',\n",
              " 23: 'cacao',\n",
              " 24: 'caneberries',\n",
              " 25: 'canola',\n",
              " 26: 'carnation',\n",
              " 27: 'carrot',\n",
              " 28: 'cassava',\n",
              " 29: 'cattleya',\n",
              " 30: 'chickpea',\n",
              " 31: 'cineraria',\n",
              " 32: 'citrus',\n",
              " 33: 'coconut palm',\n",
              " 34: 'coffee',\n",
              " 35: 'common bean',\n",
              " 36: 'cotton',\n",
              " 37: 'crucifer',\n",
              " 38: 'cucurbit',\n",
              " 39: 'cyclamen',\n",
              " 40: 'date palm',\n",
              " 41: 'durian',\n",
              " 42: 'elm',\n",
              " 43: 'flax',\n",
              " 44: 'foliage plant',\n",
              " 45: 'fuchsia',\n",
              " 46: 'geranium',\n",
              " 47: 'grape',\n",
              " 48: 'hazelnut',\n",
              " 49: 'hemp',\n",
              " 50: 'holiday cacti',\n",
              " 51: 'hop',\n",
              " 52: 'hydrangea',\n",
              " 53: 'impatiens',\n",
              " 54: 'kalanchoe',\n",
              " 55: 'lentil',\n",
              " 56: 'lettuce',\n",
              " 57: 'lisianthus',\n",
              " 58: 'maize',\n",
              " 59: 'mango',\n",
              " 60: 'mimulus, monkey-flower',\n",
              " 61: 'mint',\n",
              " 62: 'mustard',\n",
              " 63: 'oats',\n",
              " 64: 'papaya',\n",
              " 65: 'pea',\n",
              " 66: 'peach and nectarine',\n",
              " 67: 'peanut',\n",
              " 68: 'pear',\n",
              " 69: 'pearl millet',\n",
              " 70: 'pecan',\n",
              " 71: 'pepper',\n",
              " 72: 'pigeonpea',\n",
              " 73: 'pineapple',\n",
              " 74: 'pistachio',\n",
              " 75: 'pocketbook plant',\n",
              " 76: 'poinsettia',\n",
              " 77: 'potato',\n",
              " 78: 'primula',\n",
              " 79: 'red clover',\n",
              " 80: 'rhododendron',\n",
              " 81: 'rice',\n",
              " 82: 'rose',\n",
              " 83: 'rye',\n",
              " 84: 'safflower',\n",
              " 85: 'sapphire flower',\n",
              " 86: 'sorghum',\n",
              " 87: 'soybean',\n",
              " 88: 'spinach',\n",
              " 89: 'strawberry',\n",
              " 90: 'sugarcane',\n",
              " 91: 'sunflower',\n",
              " 92: 'sweetgum',\n",
              " 93: 'sweetpotato',\n",
              " 94: 'sycamore',\n",
              " 95: 'tea',\n",
              " 96: 'tobacco',\n",
              " 97: 'tomato',\n",
              " 98: 'verbena',\n",
              " 99: 'wheat',\n",
              " 100: 'wild rice'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_id = []\n",
        "for i in labels:\n",
        "  label_id.append(label_to_id[i])"
      ],
      "metadata": {
        "id": "inhB_qwpK0_F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids = np.zeros((len(df), 256))\n",
        "X_attn_masks = np.zeros((len(df), 256))"
      ],
      "metadata": {
        "id": "3otxkAAsNNLh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(df, ids, masks, tokenizer):\n",
        "    for i, text in enumerate(df['text']):\n",
        "        tokenized_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=256, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            add_special_tokens=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        ids[i, :] = tokenized_text.input_ids\n",
        "        masks[i, :] = tokenized_text.attention_mask\n",
        "    return ids, masks"
      ],
      "metadata": {
        "id": "xbVol5-6LWzK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
      ],
      "metadata": {
        "id": "38j-eEZCNMCk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels2 = np.zeros((len(df), len(set(label_id))))"
      ],
      "metadata": {
        "id": "u_DVG2qjNMFo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels2[np.arange(len(df)), label_id] = 1"
      ],
      "metadata": {
        "id": "WabZWYatNMHz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels2))\n",
        "dataset.take(196)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6JUW5o4NMJU",
        "outputId": "2419e91a-4bbd-4c87-f5ad-daaeb33ce3d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(101,), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TCDatasetMapFunction(input_ids, attn_masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attn_masks\n",
        "    }, labels"
      ],
      "metadata": {
        "id": "ERokfHxlNMLl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(TCDatasetMapFunction)"
      ],
      "metadata": {
        "id": "r00V566sQg_9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(196)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fszNwSedQhCb",
        "outputId": "881052bc-7f83-42b9-e523-893ccfa444ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(101,), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(2000).batch(16, drop_remainder=True)"
      ],
      "metadata": {
        "id": "q_HBNkoxQhEX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.8\n",
        "train_size = int((len(df)//16)*p)"
      ],
      "metadata": {
        "id": "QG2CiSGkQhGY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)"
      ],
      "metadata": {
        "id": "fQFhof_NQhIj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel"
      ],
      "metadata": {
        "id": "m-RUtTXhQhLi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rajxt2KLQhNv",
        "outputId": "e426836e-4787-4406-c2cd-196ba8da5c21"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
        "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "output_layer = tf.keras.layers.Dense(len(set(label_id)), activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
        "\n",
        "tc_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "tc_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yhV4vH3QhQR",
        "outputId": "7913175a-7e24-4938-eca7-37741267262d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " intermediate_layer (Dense)     (None, 512)          393728      ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 101)          51813       ['intermediate_layer[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,755,813\n",
            "Trainable params: 108,755,813\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "giCnBQUfQhS2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])"
      ],
      "metadata": {
        "id": "zey26DYqQhVe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = tc_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FImn1v8gQhXx",
        "outputId": "1fc5eb9e-53e8-4072-86ff-a7b818042a18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "97/97 [==============================] - 145s 1s/step - loss: 4.6952 - accuracy: 0.0129 - val_loss: 4.5530 - val_accuracy: 0.0350\n",
            "Epoch 2/32\n",
            "97/97 [==============================] - 95s 981ms/step - loss: 4.5254 - accuracy: 0.0387 - val_loss: 4.4934 - val_accuracy: 0.0250\n",
            "Epoch 3/32\n",
            "97/97 [==============================] - 95s 975ms/step - loss: 4.4717 - accuracy: 0.0387 - val_loss: 4.4382 - val_accuracy: 0.0350\n",
            "Epoch 4/32\n",
            "97/97 [==============================] - 97s 997ms/step - loss: 4.4529 - accuracy: 0.0322 - val_loss: 4.4295 - val_accuracy: 0.0400\n",
            "Epoch 5/32\n",
            "97/97 [==============================] - 95s 978ms/step - loss: 4.4349 - accuracy: 0.0374 - val_loss: 4.4393 - val_accuracy: 0.0300\n",
            "Epoch 6/32\n",
            "97/97 [==============================] - 95s 980ms/step - loss: 4.3945 - accuracy: 0.0451 - val_loss: 4.3518 - val_accuracy: 0.0550\n",
            "Epoch 7/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 4.3982 - accuracy: 0.0522 - val_loss: 4.3240 - val_accuracy: 0.0750\n",
            "Epoch 8/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 4.3296 - accuracy: 0.0638 - val_loss: 4.2671 - val_accuracy: 0.0525\n",
            "Epoch 9/32\n",
            "97/97 [==============================] - 94s 968ms/step - loss: 4.2520 - accuracy: 0.0741 - val_loss: 4.1773 - val_accuracy: 0.0700\n",
            "Epoch 10/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 4.1829 - accuracy: 0.0799 - val_loss: 4.0390 - val_accuracy: 0.0900\n",
            "Epoch 11/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 4.0959 - accuracy: 0.0947 - val_loss: 3.9993 - val_accuracy: 0.0950\n",
            "Epoch 12/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 4.0220 - accuracy: 0.1134 - val_loss: 3.8904 - val_accuracy: 0.1375\n",
            "Epoch 13/32\n",
            "97/97 [==============================] - 94s 975ms/step - loss: 3.9633 - accuracy: 0.1263 - val_loss: 3.8763 - val_accuracy: 0.1450\n",
            "Epoch 14/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 3.8758 - accuracy: 0.1450 - val_loss: 3.7649 - val_accuracy: 0.1700\n",
            "Epoch 15/32\n",
            "97/97 [==============================] - 95s 975ms/step - loss: 3.8026 - accuracy: 0.1521 - val_loss: 3.5983 - val_accuracy: 0.2150\n",
            "Epoch 16/32\n",
            "97/97 [==============================] - 94s 969ms/step - loss: 3.7344 - accuracy: 0.1643 - val_loss: 3.6665 - val_accuracy: 0.1575\n",
            "Epoch 17/32\n",
            "97/97 [==============================] - 97s 1s/step - loss: 3.6804 - accuracy: 0.1733 - val_loss: 3.5728 - val_accuracy: 0.2025\n",
            "Epoch 18/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.6281 - accuracy: 0.1811 - val_loss: 3.3627 - val_accuracy: 0.2400\n",
            "Epoch 19/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.5606 - accuracy: 0.1849 - val_loss: 3.3649 - val_accuracy: 0.2175\n",
            "Epoch 20/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 3.5149 - accuracy: 0.1952 - val_loss: 3.3359 - val_accuracy: 0.2300\n",
            "Epoch 21/32\n",
            "97/97 [==============================] - 97s 1s/step - loss: 3.4510 - accuracy: 0.2043 - val_loss: 3.2843 - val_accuracy: 0.2400\n",
            "Epoch 22/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.4093 - accuracy: 0.2197 - val_loss: 3.2070 - val_accuracy: 0.2400\n",
            "Epoch 23/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 3.3584 - accuracy: 0.2197 - val_loss: 3.2385 - val_accuracy: 0.2225\n",
            "Epoch 24/32\n",
            "97/97 [==============================] - 95s 978ms/step - loss: 3.3370 - accuracy: 0.2146 - val_loss: 3.1886 - val_accuracy: 0.2375\n",
            "Epoch 25/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 3.3068 - accuracy: 0.2062 - val_loss: 3.1875 - val_accuracy: 0.2400\n",
            "Epoch 26/32\n",
            "97/97 [==============================] - 95s 980ms/step - loss: 3.2499 - accuracy: 0.2262 - val_loss: 2.9855 - val_accuracy: 0.2550\n",
            "Epoch 27/32\n",
            "97/97 [==============================] - 95s 975ms/step - loss: 3.1904 - accuracy: 0.2352 - val_loss: 3.0646 - val_accuracy: 0.2625\n",
            "Epoch 28/32\n",
            "97/97 [==============================] - 97s 997ms/step - loss: 3.1487 - accuracy: 0.2332 - val_loss: 2.9733 - val_accuracy: 0.3025\n",
            "Epoch 29/32\n",
            "97/97 [==============================] - 94s 972ms/step - loss: 3.1737 - accuracy: 0.2281 - val_loss: 3.0158 - val_accuracy: 0.2575\n",
            "Epoch 30/32\n",
            "97/97 [==============================] - 94s 970ms/step - loss: 3.1568 - accuracy: 0.2262 - val_loss: 2.9930 - val_accuracy: 0.2575\n",
            "Epoch 31/32\n",
            "97/97 [==============================] - 94s 970ms/step - loss: 3.0937 - accuracy: 0.2423 - val_loss: 2.9767 - val_accuracy: 0.2775\n",
            "Epoch 32/32\n",
            "97/97 [==============================] - 97s 1s/step - loss: 3.0426 - accuracy: 0.2474 - val_loss: 2.9614 - val_accuracy: 0.2425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
        "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "intermediate_layer2 = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer2')(intermediate_layer)\n",
        "output_layer = tf.keras.layers.Dense(len(set(label_id)), activation='softmax', name='output_layer')(intermediate_layer2) # softmax -> calcs probs of classes\n",
        "\n",
        "tc_model2 = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "tc_model2.summary()\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "tc_model2.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
        "hist = tc_model2.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weqMgM6tasD_",
        "outputId": "921a5576-a607-43c4-a96c-a6e28d0ba833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " intermediate_layer (Dense)     (None, 512)          393728      ['bert[2][1]']                   \n",
            "                                                                                                  \n",
            " intermediate_layer2 (Dense)    (None, 512)          262656      ['intermediate_layer[0][0]']     \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 101)          51813       ['intermediate_layer2[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,018,469\n",
            "Trainable params: 109,018,469\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/32\n",
            "97/97 [==============================] - 140s 1s/step - loss: 4.5592 - accuracy: 0.0380 - val_loss: 4.3743 - val_accuracy: 0.0825\n",
            "Epoch 2/32\n",
            "97/97 [==============================] - 95s 982ms/step - loss: 4.3033 - accuracy: 0.1057 - val_loss: 4.1652 - val_accuracy: 0.1375\n",
            "Epoch 3/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 4.1378 - accuracy: 0.1327 - val_loss: 4.0466 - val_accuracy: 0.1425\n",
            "Epoch 4/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.9913 - accuracy: 0.1649 - val_loss: 3.8451 - val_accuracy: 0.1850\n",
            "Epoch 5/32\n",
            "97/97 [==============================] - 95s 982ms/step - loss: 3.8743 - accuracy: 0.1862 - val_loss: 3.7096 - val_accuracy: 0.2075\n",
            "Epoch 6/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.7561 - accuracy: 0.1972 - val_loss: 3.6493 - val_accuracy: 0.1925\n",
            "Epoch 7/32\n",
            "97/97 [==============================] - 97s 1s/step - loss: 3.6692 - accuracy: 0.1978 - val_loss: 3.5123 - val_accuracy: 0.2050\n",
            "Epoch 8/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.6046 - accuracy: 0.2101 - val_loss: 3.4272 - val_accuracy: 0.2150\n",
            "Epoch 9/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.5299 - accuracy: 0.2004 - val_loss: 3.3812 - val_accuracy: 0.2450\n",
            "Epoch 10/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 3.4581 - accuracy: 0.2126 - val_loss: 3.3886 - val_accuracy: 0.2325\n",
            "Epoch 11/32\n",
            "97/97 [==============================] - 94s 975ms/step - loss: 3.3765 - accuracy: 0.2345 - val_loss: 3.1603 - val_accuracy: 0.2550\n",
            "Epoch 12/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 3.3970 - accuracy: 0.2094 - val_loss: 3.2151 - val_accuracy: 0.2600\n",
            "Epoch 13/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 3.3274 - accuracy: 0.2133 - val_loss: 3.2285 - val_accuracy: 0.2350\n",
            "Epoch 14/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.2612 - accuracy: 0.2358 - val_loss: 3.2333 - val_accuracy: 0.2200\n",
            "Epoch 15/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 3.2492 - accuracy: 0.2397 - val_loss: 3.1986 - val_accuracy: 0.2275\n",
            "Epoch 16/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.1867 - accuracy: 0.2365 - val_loss: 3.0397 - val_accuracy: 0.2775\n",
            "Epoch 17/32\n",
            "97/97 [==============================] - 95s 980ms/step - loss: 3.1403 - accuracy: 0.2461 - val_loss: 3.1704 - val_accuracy: 0.2200\n",
            "Epoch 18/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 3.1454 - accuracy: 0.2403 - val_loss: 3.0422 - val_accuracy: 0.2575\n",
            "Epoch 19/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.0953 - accuracy: 0.2455 - val_loss: 2.9514 - val_accuracy: 0.2750\n",
            "Epoch 20/32\n",
            "97/97 [==============================] - 98s 1s/step - loss: 3.0953 - accuracy: 0.2365 - val_loss: 3.0170 - val_accuracy: 0.2375\n",
            "Epoch 21/32\n",
            "97/97 [==============================] - 95s 978ms/step - loss: 3.0684 - accuracy: 0.2378 - val_loss: 2.9881 - val_accuracy: 0.2350\n",
            "Epoch 22/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.0312 - accuracy: 0.2506 - val_loss: 3.0077 - val_accuracy: 0.2400\n",
            "Epoch 23/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 3.0265 - accuracy: 0.2423 - val_loss: 2.9357 - val_accuracy: 0.2625\n",
            "Epoch 24/32\n",
            "97/97 [==============================] - 95s 980ms/step - loss: 3.0150 - accuracy: 0.2365 - val_loss: 2.8302 - val_accuracy: 0.2925\n",
            "Epoch 25/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 2.9905 - accuracy: 0.2365 - val_loss: 2.9339 - val_accuracy: 0.2550\n",
            "Epoch 26/32\n",
            "97/97 [==============================] - 95s 975ms/step - loss: 2.9541 - accuracy: 0.2494 - val_loss: 2.7716 - val_accuracy: 0.2825\n",
            "Epoch 27/32\n",
            "97/97 [==============================] - 94s 971ms/step - loss: 2.9517 - accuracy: 0.2403 - val_loss: 2.7481 - val_accuracy: 0.2675\n",
            "Epoch 28/32\n",
            "97/97 [==============================] - 94s 971ms/step - loss: 2.9408 - accuracy: 0.2384 - val_loss: 2.7946 - val_accuracy: 0.2600\n",
            "Epoch 29/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 2.9353 - accuracy: 0.2423 - val_loss: 2.9781 - val_accuracy: 0.2325\n",
            "Epoch 30/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 2.8990 - accuracy: 0.2506 - val_loss: 2.8325 - val_accuracy: 0.2525\n",
            "Epoch 31/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 2.8871 - accuracy: 0.2358 - val_loss: 2.8592 - val_accuracy: 0.2775\n",
            "Epoch 32/32\n",
            "97/97 [==============================] - ETA: 0s - loss: 2.8720 - accuracy: 0.2378"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
        "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "intermediate_layer2 = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer2')(intermediate_layer)\n",
        "intermediate_layer3 = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer3')(intermediate_layer2)\n",
        "output_layer = tf.keras.layers.Dense(len(set(label_id)), activation='softmax', name='output_layer')(intermediate_layer3) # softmax -> calcs probs of classes\n",
        "\n",
        "tc_model3 = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "tc_model3.summary()\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "tc_model3.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
        "hist = tc_model3.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQop9phHasGd",
        "outputId": "33fe2ba8-d750-4623-b499-21f4a3bb4d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " intermediate_layer (Dense)     (None, 512)          393728      ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " intermediate_layer2 (Dense)    (None, 512)          262656      ['intermediate_layer[0][0]']     \n",
            "                                                                                                  \n",
            " intermediate_layer3 (Dense)    (None, 512)          262656      ['intermediate_layer2[0][0]']    \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 101)          51813       ['intermediate_layer3[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,281,125\n",
            "Trainable params: 109,281,125\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/32\n",
            "97/97 [==============================] - 142s 990ms/step - loss: 4.5974 - accuracy: 0.0226 - val_loss: 4.5378 - val_accuracy: 0.0450\n",
            "Epoch 2/32\n",
            "97/97 [==============================] - 94s 971ms/step - loss: 4.5147 - accuracy: 0.0213 - val_loss: 4.4638 - val_accuracy: 0.0450\n",
            "Epoch 3/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 4.4656 - accuracy: 0.0425 - val_loss: 4.4143 - val_accuracy: 0.0325\n",
            "Epoch 4/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 4.4329 - accuracy: 0.0541 - val_loss: 4.4127 - val_accuracy: 0.0575\n",
            "Epoch 5/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 4.3893 - accuracy: 0.0573 - val_loss: 4.3006 - val_accuracy: 0.0550\n",
            "Epoch 6/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 4.3407 - accuracy: 0.0638 - val_loss: 4.3044 - val_accuracy: 0.0750\n",
            "Epoch 7/32\n",
            "97/97 [==============================] - 94s 969ms/step - loss: 4.2900 - accuracy: 0.0696 - val_loss: 4.1985 - val_accuracy: 0.0775\n",
            "Epoch 8/32\n",
            "97/97 [==============================] - 95s 977ms/step - loss: 4.2521 - accuracy: 0.0728 - val_loss: 4.2118 - val_accuracy: 0.0800\n",
            "Epoch 9/32\n",
            "97/97 [==============================] - 95s 982ms/step - loss: 4.1899 - accuracy: 0.0857 - val_loss: 4.0326 - val_accuracy: 0.0850\n",
            "Epoch 10/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 4.1479 - accuracy: 0.0773 - val_loss: 4.1033 - val_accuracy: 0.1025\n",
            "Epoch 11/32\n",
            "97/97 [==============================] - 94s 973ms/step - loss: 4.0948 - accuracy: 0.0973 - val_loss: 4.0401 - val_accuracy: 0.0925\n",
            "Epoch 12/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 4.0521 - accuracy: 0.1005 - val_loss: 3.9625 - val_accuracy: 0.1150\n",
            "Epoch 13/32\n",
            "97/97 [==============================] - 95s 981ms/step - loss: 3.9988 - accuracy: 0.1179 - val_loss: 3.8476 - val_accuracy: 0.1625\n",
            "Epoch 14/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.9517 - accuracy: 0.1186 - val_loss: 3.8048 - val_accuracy: 0.1450\n",
            "Epoch 15/32\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 3.8875 - accuracy: 0.1347 - val_loss: 3.8061 - val_accuracy: 0.1500\n",
            "Epoch 16/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.8284 - accuracy: 0.1572 - val_loss: 3.6688 - val_accuracy: 0.1950\n",
            "Epoch 17/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.7803 - accuracy: 0.1579 - val_loss: 3.6818 - val_accuracy: 0.1750\n",
            "Epoch 18/32\n",
            "97/97 [==============================] - 98s 1s/step - loss: 3.7032 - accuracy: 0.1727 - val_loss: 3.6506 - val_accuracy: 0.1525\n",
            "Epoch 19/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.6649 - accuracy: 0.1843 - val_loss: 3.5353 - val_accuracy: 0.2025\n",
            "Epoch 20/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.6131 - accuracy: 0.1927 - val_loss: 3.4653 - val_accuracy: 0.2300\n",
            "Epoch 21/32\n",
            "97/97 [==============================] - 94s 971ms/step - loss: 3.5505 - accuracy: 0.1985 - val_loss: 3.3966 - val_accuracy: 0.2375\n",
            "Epoch 22/32\n",
            "97/97 [==============================] - 94s 974ms/step - loss: 3.5087 - accuracy: 0.2101 - val_loss: 3.4701 - val_accuracy: 0.2150\n",
            "Epoch 23/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.4664 - accuracy: 0.2068 - val_loss: 3.3322 - val_accuracy: 0.2450\n",
            "Epoch 24/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.4160 - accuracy: 0.2068 - val_loss: 3.2966 - val_accuracy: 0.2475\n",
            "Epoch 25/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.4061 - accuracy: 0.2004 - val_loss: 3.2796 - val_accuracy: 0.2625\n",
            "Epoch 26/32\n",
            "97/97 [==============================] - 95s 976ms/step - loss: 3.3357 - accuracy: 0.2229 - val_loss: 3.2176 - val_accuracy: 0.2425\n",
            "Epoch 27/32\n",
            "97/97 [==============================] - 94s 970ms/step - loss: 3.3127 - accuracy: 0.2326 - val_loss: 3.2123 - val_accuracy: 0.2400\n",
            "Epoch 28/32\n",
            "97/97 [==============================] - 94s 972ms/step - loss: 3.2566 - accuracy: 0.2229 - val_loss: 3.2134 - val_accuracy: 0.2375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fQkmC8hasKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ud4b8WQasLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}